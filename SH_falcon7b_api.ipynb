{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPWC5phQ62g04KC1CoV5zRW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/falon-go-weeee/Colab-Notebooks/blob/main/SH_falcon7b_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J_8AkXl-uDf",
        "outputId": "05341176-871c-4e31-8235-cf8eb8bdf17a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required Libraries"
      ],
      "metadata": {
        "id": "zzDQuCnTijqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate einops xformers fastapi[all] uvicorn[standard]"
      ],
      "metadata": {
        "id": "ypeXsY_pjtGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from subprocess import getoutput\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "import time\n",
        "\n",
        "Tunnel_type = \"bore\" #@param [\"ngrok\", \"bore\", \"cloudflare\"]\n",
        "binding_port = \"8000\" #@param {type:\"string\"}\n",
        "port = \"34525\" #@param {type:\"string\"}\n",
        "\n",
        "def install_ngrok():\n",
        "  if not os.path.exists('/usr/local/lib/python3.8/dist-packages/pyngrok'):\n",
        "    !pip install pyngrok\n",
        "    clear_output()\n",
        "\n",
        "def install_bore():\n",
        "  if not os.path.exists('/usr/bin/bore'):\n",
        "    !wget https://github.com/ekzhang/bore/releases/download/v0.4.1/bore-v0.4.1-i686-unknown-linux-musl.tar.gz\n",
        "    !tar -xf bore-v0.4.1-i686-unknown-linux-musl.tar.gz\n",
        "    !rm -f bore-v0.4.1-i686-unknown-linux-musl.tar.gz\n",
        "    !cp bore /usr/bin/bore\n",
        "    !rm -rf bore\n",
        "    clear_output()\n",
        "\n",
        "def install_cloudflare():\n",
        "  if not os.path.exists('/usr/bin/cloudflared'):\n",
        "    !wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb && dpkg -i cloudflared-linux-amd64.deb\n",
        "    !rm -f cloudflared-linux-amd64.deb\n",
        "    clear_output()\n",
        "\n",
        "if Tunnel_type == \"bore\":\n",
        "  install_bore()\n",
        "  use_ip = \"\"\n",
        "  if port:\n",
        "    # !curl ifconfig.me > ip.txt\n",
        "    # clear_output()\n",
        "    # ip = getoutput('cat /content/ip.txt')\n",
        "    # ip = f\"-l {ip}\"\n",
        "    !nohup bore local $binding_port --to bore.pub -p $port > /content/tunnel_srv.txt 2>&1 &\n",
        "  else:\n",
        "    !nohup bore local $binding_port --to bore.pub > /content/tunnel_srv.txt 2>&1 &\n",
        "  time.sleep(3)\n",
        "  !grep -o 'bore.pub:[^ ]*' /content/tunnel_srv.txt >/content/tunnel_srvr.txt\n",
        "  time.sleep(3)\n",
        "  tunnel_srv= getoutput('cat /content/tunnel_srvr.txt')\n",
        "  if tunnel_srv == '':\n",
        "    print('port not available choose another port')\n",
        "  else:\n",
        "    print('http://'+tunnel_srv)\n",
        "  !rm /content/tunnel_srv.txt\n",
        "  !rm /content/tunnel_srvr.txt\n",
        "\n",
        "elif Tunnel_type == \"ngrok\":\n",
        "  install_ngrok()\n",
        "  from pyngrok import ngrok\n",
        "  # Terminate ngrok port\n",
        "  ngrok.kill()\n",
        "  # Set authentication (optional)\n",
        "  # Get your authentication token via https://dashboard.ngrok.com/auth\n",
        "  NGROK_AUTH_TOKEN = \"2H79LWzHBonanC1xWxoGXW6fvtY_5rD7uShKgACuSLvUwMvJA\"\n",
        "  ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "  public_url = ngrok.connect(binding_port)\n",
        "  print(public_url)\n",
        "\n",
        "elif Tunnel_type == \"cloudflare\":\n",
        "  install_cloudflare()\n",
        "  !pkill cloudflared\n",
        "  time.sleep(4)\n",
        "  # Tunnel_link = \"& cloudflared tunnel --url http://localhost:7860\"\n",
        "  !nohup cloudflared tunnel --url http://localhost:$binding_port > /content/srv.txt 2>&1 &\n",
        "  time.sleep(4)\n",
        "  !grep -o 'https[^[:space:]]*\\.trycloudflare.com' /content/srv.txt >/content/srvr.txt\n",
        "  srv= getoutput('cat /content/srvr.txt')\n",
        "  print(srv)\n",
        "  time.sleep(4)\n",
        "  !rm /content/srv.txt /content/srvr.txt"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Tp57w7f6crM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37fafba6-88b9-4640-8823-662df14deb22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://bore.pub:34525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -u \"/content/drive/MyDrive/Colab Notebooks/falcon-7b-api.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvY2L3s7BMVl",
        "outputId": "78417896-51ac-4cc8-94d1-fa01ad62c30b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-11 07:30:40.823829: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading checkpoint shards: 100% 2/2 [01:18<00:00, 39.47s/it]\n",
            "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m5433\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:54926 - \"\u001b[1mGET /falcon?prompt=how%20to%20run%20a%20query HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:42352 - \"\u001b[1mGET /falcon?prompt=write%20a%20postgresql%20sqlqlchemy%20query%20to%20get%20data%20from%20two%20tables%20from%20the%20same%20schema HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## kill process\n",
        "\n",
        "import os\n",
        "import signal\n",
        "import psutil\n",
        "\n",
        "process_name = \"bore\" #@param {type:\"string\"}\n",
        "\n",
        "for proc in psutil.process_iter(['pid', 'name']):\n",
        "  if proc.info['name'] == process_name:\n",
        "      os.kill(proc.pid, signal.SIGTERM)\n",
        "!ps -aux | grep $process_name"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YiBfEPiWMayx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}